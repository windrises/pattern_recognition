{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_labels = pd.read_csv('./test_labels.csv')\n",
    "subm = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import swifter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import TweetTokenizer, TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "from textblob import Word\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tknzr = TweetTokenizer()\n",
    "    for word, pos in pos_tag(tknzr.tokenize(sentence)):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    return res\n",
    "\n",
    "\n",
    "sp = SpellCorrector(corpus=\"english\") \n",
    "seg_tw = Segmenter(corpus=\"twitter\")\n",
    "cnt = 0\n",
    "all_words = {}\n",
    "def clean_text(text):\n",
    "    global cnt\n",
    "    cnt += 1\n",
    "#     if cnt >= 5005 and cnt <= 5008:\n",
    "#         print(text)\n",
    "#     else:\n",
    "#         return ' '\n",
    "#     print(text)\n",
    "#     print(cnt)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(cnt)\n",
    "#     x_ascii = unidecode(x)\n",
    "#     x_clean = special_character_removal.sub('',x_ascii)\n",
    "#                 '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    text = text.lower()\n",
    "    text = re.sub('[\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\\\t\\\\n]', ' ', text)\n",
    "    text = text.replace(\"â€™\", \"'\")\n",
    "    \n",
    "    p = [\"'ll\",   \"can't\",   \"n't\",  \"'m\",  \"'s\", \"'ve\",   \"'re\",  \"'d\"]\n",
    "    q = [\" will\", \"can not\", \" not\", \" am\", \"\",   \" have\", \" are\", \"would\"]\n",
    "    for i in range(len(p)):\n",
    "        text = text.replace(p[i], q[i])\n",
    "        \n",
    "    words = lemmatize_sentence(text)\n",
    "\n",
    "    segment_words = []\n",
    "    for x in words:\n",
    "        if len(x) > 50:\n",
    "            segment_words.append(x)\n",
    "            continue\n",
    "        if x in all_words:\n",
    "            segment_words.extend(all_words[x])\n",
    "        else:\n",
    "            if x in embeddings_index_ft:\n",
    "                segment_words.append(x)\n",
    "                all_words[x] = [x]\n",
    "            else:\n",
    "                x = sp.correct(x)\n",
    "                if x in embeddings_index_ft:\n",
    "                    segment_words.append(x)\n",
    "                    all_words[x] = [x]\n",
    "                else:\n",
    "                    try:\n",
    "                        seg_word = seg_tw.segment(x)\n",
    "                        seg_word = seg_word.split(' ')\n",
    "                        segment_words.extend(seg_word)\n",
    "                        all_words[x] = seg_word\n",
    "                    except:\n",
    "                        segment_words.append(x)\n",
    "                        all_words[x] = [x]\n",
    "    clean_text = ' '.join(segment_words)\n",
    "    return clean_text\n",
    "\n",
    "train['clean_text'] = train['comment_text'].swifter.set_npartitions(64).apply(lambda x: clean_text(str(x)))\n",
    "test['clean_text'] = test['comment_text'].swifter.set_npartitions(64).apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "train['clean_text'] = train['clean_text'].fillna('something')\n",
    "test['clean_text'] = test['clean_text'].fillna('something')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    \n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    \n",
    "    df['polarity'] = df['comment_text'].swifter.set_npartitions(64).apply(lambda x:TextBlob(str(x)).sentiment.polarity)\n",
    "    df['subjectivity'] = df['comment_text'].swifter.set_npartitions(64).apply(lambda x:TextBlob(str(x)).sentiment.subjectivity)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "\n",
    "feature_name = ['caps_vs_length', 'words_vs_unique', 'polarity', 'subjectivity']\n",
    "features = train[feature_name].fillna(0)\n",
    "test_features = test[feature_name].fillna(0)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('./train_100.csv')\n",
    "test.to_csv('./test_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train['clean_text']\n",
    "x_test = test['clean_text']\n",
    "label_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y_train = train[label_names].values\n",
    "y_test = test_labels[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# reader = tf.train.NewCheckpointReader('./multi_cased_L-12_H-768_A-12/bert_model.ckpt')\n",
    "# tensor = reader.get_tensor('bert/embeddings/word_embeddings')\n",
    "# print(type(tensor), tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f  = open('./multi_cased_L-12_H-768_A-12/vocab.txt')\n",
    "# embeddings_index = {}\n",
    "# for i, line in enumerate(f):\n",
    "#     line = line.strip('\\n')\n",
    "#     embeddings_index[line] = tensor[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "max_features = 300000\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "maxlen = 1000\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_test = test['comment_text']\n",
    "# sub_test = tokenizer.texts_to_sequences(sub_test)\n",
    "# sub_test = sequence.pad_sequences(sub_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FastText Web Crawl vectors\n",
    "EMBEDDING_FILE_FASTTEXT=\"./crawl-300d-2M.vec\"\n",
    "EMBEDDING_FILE_TWITTER=\"./glove.twitter.27B.200d.txt\"\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index_ft = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))\n",
    "embeddings_index_tw = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE_TWITTER,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = spell_model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word):\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word):\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word):\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words):\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "\n",
    "# seg_tw = Segmenter(corpus=\"twitter\")\n",
    "embedding_size = 501\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words,501))\n",
    "\n",
    "something_tw = embeddings_index_tw.get(\"something\")\n",
    "something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((501,))\n",
    "something[:300,] = something_ft\n",
    "something[300:500,] = something_tw\n",
    "something[500,] = 0\n",
    "\n",
    "def all_caps(word):\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix,i,word):\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "    if embedding_vector_ft is not None: \n",
    "        if all_caps(word):\n",
    "            last_value = np.array([1])\n",
    "        else:\n",
    "            last_value = np.array([0])\n",
    "        embedding_matrix[i,:300] = embedding_vector_ft\n",
    "        embedding_matrix[i,500] = last_value\n",
    "        embedding_vector_tw = embeddings_index_tw.get(word)\n",
    "        if embedding_vector_tw is not None:\n",
    "            embedding_matrix[i,300:500] = embedding_vector_tw\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if i >= max_features: continue\n",
    "        \n",
    "    if embeddings_index_ft.get(word) is not None:\n",
    "        embed_word(embedding_matrix,i,word)\n",
    "    else:\n",
    "        if len(word) > 20:\n",
    "            embedding_matrix[i] = something \n",
    "        else:\n",
    "            word2 = correction(word)\n",
    "            if embeddings_index_ft.get(word2) is not None:\n",
    "                embed_word(embedding_matrix,i,word2)\n",
    "            else:\n",
    "                word2 = correction(singlify(word))\n",
    "                if embeddings_index_ft.get(word2) is not None:\n",
    "                    embed_word(embedding_matrix,i,word2)\n",
    "                else:\n",
    "                    embedding_matrix[i] = something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import log_loss\n",
    "import keras\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "tag = ''\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global tag\n",
    "        print(epoch)\n",
    "        epoch = str(epoch)\n",
    "        preds = self.model.predict([x_test, test_features], batch_size=64, verbose=1)\n",
    "        print(preds.shape)\n",
    "        \n",
    "        submid = pd.DataFrame({'id': subm['id']})\n",
    "        submission = pd.concat([submid, pd.DataFrame(preds, columns=label_names)], axis=1)\n",
    "        submission.to_csv(tag + '/submission' + tag + '_' + epoch + '.csv', index=False)\n",
    "\n",
    "        sub = pd.read_csv(tag + '/submission' + tag + '_' + epoch + '.csv')\n",
    "        avg_score = 0.0\n",
    "        for i, label_name in enumerate(label_names):\n",
    "            y_last = test_labels[label_name].values\n",
    "\n",
    "            idx = y_last != -1\n",
    "            score = mean_squared_log_error(y_last[idx], sub[label_name][idx])\n",
    "            avg_score += score\n",
    "            print(label_name + ': ' + str(score))\n",
    "        print(avg_score/len(label_names))\n",
    "        print(roc_auc_score(test_labels[label_names][idx], sub[label_names][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, Bidirectional, GRU, SpatialDropout1D, CuDNNLSTM, CuDNNGRU, concatenate\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "tag = '101'\n",
    "try:\n",
    "    os.mkdir(tag)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "history = LossHistory()\n",
    "checkpoint = ModelCheckpoint(tag + '/{epoch:02d}-{val_loss:.4f}.model', monitor='val_acc', verbose=1, period=1)\n",
    "callbacks_list = [history]\n",
    "# Embedding\n",
    "# max_features = 20000\n",
    "# maxlen = 300\n",
    "# embedding_size = 512\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "# # print('Loading data...')\n",
    "# # (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# print(len(X_train), 'train sequences')\n",
    "# # print(len(x_test), 'test sequences')\n",
    "\n",
    "# print('Pad sequences (samples x time)')\n",
    "# X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "# # x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# print('x_train shape:', X_train.shape)\n",
    "# # print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "l0 = Input(shape=(maxlen,))\n",
    "features_input = Input(shape=(features.shape[1],))\n",
    "# l1 = Embedding(num_words,\n",
    "#                     embedding_size,\n",
    "#                     input_length=maxlen)(l0)\n",
    "l1 = Embedding(num_words,\n",
    "                    embedding_size,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False)(l0)\n",
    "l2 = SpatialDropout1D(0.5)(l1)\n",
    "l3 = Bidirectional(CuDNNLSTM(40, return_sequences=True))(l2)\n",
    "l4, l4_h, l4_c = Bidirectional(CuDNNGRU(40, return_sequences=True, return_state = True))(l3)\n",
    "# model.add(Dropout(0.5))\n",
    "la = GlobalAveragePooling1D()(l4)\n",
    "lm = GlobalMaxPooling1D()(l4)\n",
    "l5 = concatenate([la, l4_h, lm, features_input])\n",
    "l6 = Dense(6, activation='sigmoid')(l5)\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "print('Train...')\n",
    "\n",
    "model = Model([l0, features_input], l6)\n",
    "adam = optimizers.adam(clipvalue=1)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy', 'mean_squared_logarithmic_error'])\n",
    "model.fit([x_train, features], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1,\n",
    "#           validation_data=([x_test, test_features[idx]], y_test),\n",
    "          callbacks=callbacks_list)\n",
    "# score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
